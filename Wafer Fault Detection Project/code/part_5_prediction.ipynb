{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fc748f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data_injetsion and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb93b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e81fd6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file good_data_pred already exists.\n"
     ]
    }
   ],
   "source": [
    "mkdir good_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9007c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file Bad_data_pred already exists.\n"
     ]
    }
   ],
   "source": [
    "mkdir Bad_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ece4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLobal path varibales\n",
    "\n",
    "schema_file_path = r\"D:\\\\learnbay\\\\courses\\\\ongoing_class\\\\march_batches\\\\21st October Wkdy\\\\schema_prediction.json\"\n",
    "training_folder_path = \"D:\\\\learnbay\\\\courses\\\\ongoing_class\\\\march_batches\\\\21st October Wkdy\\\\Prediction_Batch_Files/\" \n",
    "good_data_folder_path = \"D:\\\\learnbay\\\\courses\\\\ongoing_class\\\\march_batches\\\\21st October Wkdy\\\\good_data_pred/\"\n",
    "bad_data_folder_path = \"D:\\\\learnbay\\\\courses\\\\ongoing_class\\\\march_batches\\\\21st October Wkdy\\\\Bad_data_pred/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37b4afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuesfromschema():\n",
    "    with open(schema_file_path, 'r') as f:\n",
    "        dic = json.load(f)\n",
    "        pattern = dic['SampleFileName']\n",
    "        LengthOfDateStampInFile = dic['LengthOfDateStampInFile']\n",
    "        LengthOfTimeStampInFile = dic['LengthOfTimeStampInFile']\n",
    "        column_names = dic['ColName']\n",
    "        NumberofColumns = dic['NumberofColumns']\n",
    "        \n",
    "        return pattern, LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b754de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern, LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns = valuesfromschema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d6e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex():\n",
    "    regex = \"['wafer']+['\\_'']+[\\d_]+[\\d]+\\.csv\"\n",
    "    return regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94e5653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f29974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file_name(regex, LengthOfDateStampInFile, LengthOfTimeStampInFile):\n",
    "\n",
    "    for filename in os.listdir(training_folder_path):\n",
    "        if (re.match(regex, filename)):\n",
    "            split_at_dot = re.split('.csv', filename)\n",
    "            split_at_dot = re.split('_', split_at_dot[0])\n",
    "            if len(split_at_dot[1]) == LengthOfDateStampInFile:\n",
    "                if len(split_at_dot[2]) == LengthOfTimeStampInFile: \n",
    "                    shutil.copy(f'{training_folder_path}'+filename , good_data_folder_path)\n",
    "                else:\n",
    "                    shutil.copy(f'{training_folder_path}'+filename , bad_data_folder_path)\n",
    "            else:\n",
    "                shutil.copy(f'{training_folder_path}'+filename , bad_data_folder_path)\n",
    "        else:\n",
    "            shutil.copy(f'{training_folder_path}'+filename , bad_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7de72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_file_name(regex, LengthOfDateStampInFile, LengthOfTimeStampInFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ec6dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating column length in the file\n",
    "def validatecolumn(NumberofColumns):\n",
    "    for filename in os.listdir(good_data_folder_path):\n",
    "        csv = pd.read_csv(f'{good_data_folder_path}'+filename)\n",
    "        if csv.shape[1] == NumberofColumns:\n",
    "            pass\n",
    "        else:\n",
    "            shutil.copy(f'{good_data_folder_path}'+filename , bad_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a47460",
   "metadata": {},
   "outputs": [],
   "source": [
    "validatecolumn(NumberofColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7c316fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate missing value in entire column\n",
    "def validate_missing_column():\n",
    "    for filename in os.listdir(good_data_folder_path):\n",
    "        csv = pd.read_csv(f'{good_data_folder_path}'+filename)\n",
    "        count=0\n",
    "        for column in csv:\n",
    "            if (len(csv[column])- csv[column].count()) == len(csv[column]):\n",
    "                count +=1\n",
    "                shutil.copy(f'{good_data_folder_path}'+filename , bad_data_folder_path)\n",
    "                break\n",
    "        if count==0:\n",
    "            csv.rename(columns = {'Unnamed: 0': \"wafer\"}, inplace = True)\n",
    "            csv.to_csv(f'{good_data_folder_path}'+ filename, index = None , header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a7e0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_missing_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e6d94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data in 1 csv file\n",
    "\n",
    "li = []\n",
    "for filename in os.listdir(good_data_folder_path):\n",
    "    csv = pd.read_csv(f'{good_data_folder_path}'+filename)\n",
    "    li.append(csv)\n",
    "\n",
    "combined_data = pd.concat(li , axis = 0, ignore_index=True)\n",
    "combined_data.to_csv('pred_main_data.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d7a6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Data preprcoessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e116e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df = pd.read_csv('pred_main_data.csv')\n",
    "\n",
    "df\n",
    "\n",
    "# deleting unwanted column\n",
    "df.drop(columns = ['Unnamed: 0' , 'wafer'] , inplace = True)\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3, weights='uniform' , missing_values=np.nan)\n",
    "new_data = imputer.fit_transform(df)\n",
    "\n",
    "new_data = pd.DataFrame(new_data , columns= df.columns)\n",
    "new_data.to_csv('pred_clean_combined_data.csv')\n",
    "\n",
    "\n",
    "# # checking columns whihc have standard deviation of zero\n",
    "col_to_drop = []\n",
    "describe = df.describe()\n",
    "for x in new_data.columns:\n",
    "    if describe[x]['std']==0:\n",
    "        col_to_drop.append(x)\n",
    "\n",
    "col_to_drop\n",
    "new_data.drop(columns=col_to_drop, axis=1 , inplace=True)\n",
    "\n",
    "\n",
    "new_data.to_csv('pred_clean_data.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d84f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3 - clustering\n",
    "\n",
    "#loading the model\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('pred_clean_data.csv')\n",
    "\n",
    "model = joblib.load('models/kmeans.pkl')\n",
    "y_pred = model.predict(data)\n",
    "\n",
    "data['cluster'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d1782dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pred_clusters.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca652ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part4 prediction\n",
    "\n",
    "data = pd.read_csv('pred_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8dcd8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = data['cluster'].unique()\n",
    "\n",
    "#loading all the models\n",
    "\n",
    "model_0 = joblib.load('models/random_forest_0.pkl')\n",
    "model_1 = joblib.load('models/xgboost_1.pkl')\n",
    "model_2 = joblib.load('models/random_forest_2.pkl')\n",
    "\n",
    "\n",
    "for i in clusters:\n",
    "    cluster_data = data[data['cluster']==i]\n",
    "#     wafer_names = list(data[''])\n",
    "    cluster_data = data.drop(columns='cluster' , axis = 1)\n",
    "    if i == 0:\n",
    "        predict_0 = model_0.predict(cluster_data)\n",
    "    if i == 1:\n",
    "        predict_1 = model_1.predict(cluster_data)\n",
    "    if i == 2:\n",
    "        predict_2 = model_2.predict(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7afcc60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predict_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3dff03b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ee99901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c79433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data\n",
    "\n",
    "cluster - respect - model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
